{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b808301f-b4f8-4fcd-8751-1375cf34a536",
   "metadata": {},
   "source": [
    "<h1>Big Data Overview</h1>\n",
    "<li>Explain of Handoop, MapReduce, Spark, and PySpark</li>\n",
    "<li>Local versus Distributed System</li>\n",
    "<li>Overview of Handoop Ecosystem</li>\n",
    "<li>Detailed overview of Spark</li>\n",
    "<li>Set-up on Amazon Web Services</li>\n",
    "<li>Resources on other Spark Options</li>\n",
    "<li>Jupyter Notebook hands-on code with PySpark and RDDs</li>\n",
    "<p>\n",
    "    We've worked with Data that can fit on a local computer, in the scale of 0-8 GB. But what can we do if we have a larger set of data?\n",
    "    <br>Try using a SQL database to move storage onto hard drive instead of RAM.\n",
    "    <br>Or use a distributed system, that distributes the data to multiple marchines/computer.\n",
    "</p>\n",
    "<p>\n",
    "    A local process will use the computation resources of a single machine\n",
    "    <br>A distributed process has access to the computational resources across a number of machines connected through a network\n",
    "    <br>After a certain point, it is easier to scale out to many lower CPU machines, than to try to scale up to a single machine with high a CPU.\n",
    "    <br>Distributed machines also have the advantage of easily scaling, you can just add more machines.\n",
    "    <br>They also include fault tolerance, if one machine fails, the whole network can still go on.\n",
    "    <br>Let's discuss the typical format of a distributed architecture that uses Hadoop.\n",
    "</p>\n",
    "<p>\n",
    "    Hadoop is a way to distribute very large files across multiple machines.\n",
    "    <br>It uses the Hadoop Distributed File System (HDFS). HDFS allows a user to work with large data sets and also duplicates blocks of data for fault tolerance. It also then uses MapReduce.\n",
    "    <br>MapReduce allows computations on that data\n",
    "    <br>HDFS will use blocks of data, with a size of 128 MB by default. Each of these blocks is replicated 3 times. The blocks are distributed in a way to support fault tolerance.\n",
    "    <br>Smaller blocks provide more parallelization during processing. Multiple copies of a block prevent loss of data fue to a failure of a node.\n",
    "    <br>MapReduce is a way of splitting a computation task to a distributed set of files (such as HDFS). It consist of a Job Tracker and multiple Task Trackers.\n",
    "    <br>The Job Tracker sends code to run on the Task Trackers. The Task trackers allocate CPU and memory for the tasks and monitor the tasks on the worker nodes.\n",
    "</p>\n",
    "<p>\n",
    "    What we covered can be thought of in two distinct parts:\n",
    "    <li>Using HDFS to distribute large data sets</li>\n",
    "    <li>Using MapReduce to distribute a computational task to a distributed data set</li>\n",
    "    <!-- Next we will learn about the latest technology in this space known as Spark. Spark improves on the concepts of using distribution -->\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
