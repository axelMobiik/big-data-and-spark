{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f62d157-2b02-4760-b63f-b647ccb6eb37",
   "metadata": {},
   "source": [
    "<h1>Spark Overview</h1>\n",
    "<p>\n",
    "    This lecture will be an abstract overview, we will discuss:\n",
    "    <li>Spark</li>\n",
    "    <li>Spark vs MapReduce</li>\n",
    "    <li>Spark RDDs</li>\n",
    "    <li>RDD Operations</li>\n",
    "    Don't worry about having to understand all the operations, we will review and cover this again when we actually program with Spark and Python\n",
    "</p>\n",
    "<p>\n",
    "    Spark is one of the latest technologies being used to quickly and easily handle Big Data; it is an open source project on Apache. It was first released in February 2013 and has exploded in popularity due to it's ease of use and speed. It was created at the AMPLab at UC Berkeley.\n",
    "    <br>You can think of Spark as a flexible alternative to MapReduce. Spark can use data stored in a variety of formats:\n",
    "    <li>Cassandra</li>\n",
    "    <li>AWS S3</li>\n",
    "    <li>HDFS</li>\n",
    "    <li>And more</li>\n",
    "</p>\n",
    "<p>\n",
    "    MapReduce requires files to be stored in HDFS, Spark does not! Spark also can perform operations up to 100x faster than MapReduce, so how does it achieve this speed?\n",
    "    <br>MapReduce writes most data to disk after each map and reduce operation.\n",
    "    <br>Spark keeps most of the data in memory after each transformation. Spark can spill over to disk if the memory is filled.\n",
    "</p>\n",
    "<p>\n",
    "    At the core of Spark is the idea of a Resilient Distributed Dataset (RDD)\n",
    "    <br>Resilient Distributed Dataset (RDD) has 4 main features:\n",
    "    <li>Distributed Collection of Data</li>\n",
    "    <li>Fault-tolerant</li>\n",
    "    <li>Parallel operation - partioned</li>\n",
    "    <li>Ability to use many data sources</li>\n",
    "    RDDs are immutable, lazily evaluated, and cacheable. There are two types of RDD operations:\n",
    "    <li>Transformations</li>\n",
    "    <li>Actions</li>\n",
    "    Basic Actions:\n",
    "    <li>First - Return the first element in the RDD</li>\n",
    "    <li>Collect - Return all the elements of the RDD as an array at the driver program</li>\n",
    "    <li>Count - Return the number of elements in the RDD</li>\n",
    "    <li>Take - Return an array with the firts n elements of the RDD</li>\n",
    "    Basic Transformations:\n",
    "    <li>Filter</li>\n",
    "    <li>Map</li>\n",
    "    <li>FlatMap</li>\n",
    "</p>\n",
    "<p>\n",
    "    RDD.filter():\n",
    "    <br>Applies a function to each element and returns elements that evaluate to true\n",
    "    <br>\n",
    "    <br>RDD.map():\n",
    "    <br>Transforms each element and preserves # of elements, very similar idea to pandas .apply()\n",
    "    <br>\n",
    "    <br>RDD.flatMap():\n",
    "    <br>Transform each element into 0-N elements and changes # of elements\n",
    "    <li>Map() - Grabbing first letter of a list of names</li>\n",
    "    <li>FlatMap() - Transforming a corpus of text into a list of words</li>\n",
    "    <li>We will show many more examples when programing with PySpark</li>\n",
    "</p>\n",
    "<p>\n",
    "    Often RDDs will be holding their values in tuples (key, value), this offers better partitioning of data and leads to functionality based on reduction.\n",
    "</p>\n",
    "<p>\n",
    "    Reduce():\n",
    "    <br>An action that will aggregate RDD elements using a function that returns a single element\n",
    "    <br>\n",
    "    <br>ReduceByKey():\n",
    "    <br>An action that will aggregate Pai RDD elements using a function that returns a Pair RDD\n",
    "    <br>\n",
    "    <br>These ideas are similar to a Group By operation\n",
    "</p>\n",
    "<p>\n",
    "    Spark is being continually developed and new releases come out often! The Spark Ecosystem now includes:\n",
    "    <li>Spark SQL</li>\n",
    "    <li>Spark DataFrames</li>\n",
    "    <li>MLlib</li>\n",
    "    <li>GraphX</li>\n",
    "    <li>Spark Streaming</li>\n",
    "</p>\n",
    "<p>\n",
    "    Now we've learned enough to get started! we're noe going to show you how to set up an Amazon Web Services account to get Spark up and running!\n",
    "    <br>We'll also have text article lecture for some other options in case you don't want to use AWS\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
